%! Author = idegen
%! Date = 26/11/2021

\documentclass{article}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[default,oldstyle,scale=0.95]{opensans}
\usepackage[]{amsthm}
\usepackage[]{amssymb}
\usepackage{placeins}
\usepackage{textcomp}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage[useregional]{datetime2}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=Emerald,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=RoyalBlue
}
\makeatletter
\def\namedlabel#1#2{\begingroup
#2%
\def\@currentlabel{#2}%
\phantomsection\label{#1}\endgroup
}
\makeatother

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

\pagestyle{fancy}
\fancyhf{}
\chead{Machine Learning Paradigms}
\rhead{\today}
\lhead{Isabella Degen}
\lfoot{isabella.degen@bristol.ac.uk}
\rfoot{Page \thepage}

\title{Machine Learning Paradigms - Coursework Report}
\author{Isabella Degen - Interactive AI CDT}

\begin{document}
    \maketitle


    \section{Introduction}\label{sec:introduction}
    Motivation and Approach %TODO: delete
    My goal for this coursework was to gain more experience with the various ML algorithms and evaluation methods we've
    seen during the semester, various different python frameworks available, methods to keep track of experiments, as
    well as learning more about how to apply good software engineering practices I'm used to from industry to a ML problem.
    I was less focused on finding the best solution or winning the competition.

    I followed the steps laid out on \href{https://www.kaggle.com/c/morebikes2021/overview/description}{Kaggle} and broke
    them down into the following sub steps:

    \subsection*{\label{phs:one}{Phase 1}}
    Predictions of station's 201-275 availabilities for one month by (a) training a model per station or (b)
    training one model for all stations.

    \begin{description}
        \item [\namedlabel{itm:phase1-step1}{Step 1}] Analyse and investigate station's test data and Decide on what problem it is. Classification (binary, multi label), Regression, ...
        \item [\namedlabel{itm:phase1-step2}{Step 2}] Experiment with a few models using approach a (training model per station)
        \item [\namedlabel{itm:phase1-step3}{Step 3}] Experiment with a few models using approach b (training one model for all station)
        \item [\namedlabel{itm:phase1-step4}{Step 4}] Tune the more promising potential solutions
        \item [\namedlabel{itm:phase1-step5}{Step 5}] Submit results
    \end{description}

    \subsection*{\label{phs:two}{Phase 2}}
    Use the pre-trained models for station 1-200 to predict availability for stations 201-275
    \begin{description}
        \item [\namedlabel{itm:phase2-step1}{Step 1}] Analyse given models
        \item [\namedlabel{itm:phase2-step2}{Step 2}] Experiment with a few approaches
        \item [\namedlabel{itm:phase2-step3}{Step 3}] Tune the more promising potential solutions
        \item [\namedlabel{itm:phase2-step4}{Step 4}] Investigate performance
        \item [\namedlabel{itm:phase2-step5}{Step 5}] Submit results
        \item [\namedlabel{itm:phase2-step6}{Step 6}] Compare with results from \nameref{phs:one}
    \end{description}

    \subsection*{\label{phs:three}{Phase 3}}
    Combine approaches from \nameref{phs:one} and \nameref{phs:two}
    \begin{description}
        \item [\namedlabel{itm:phase3-step1}{Step 1}] TODO

    \end{description}
    I keep referring back to these steps in section: \nameref{sec:method}, \nameref{sec:technical-background}, \nameref{sec:experiment-setup},
    \nameref{sec:results}. The final sections \nameref{sec:conclusions} gives a summary of my overall learnings and conclusions.

    %TODO reference to github
    %TODO reference interactive to weight and biases graphs, include static version


    \section{Technical Background}\label{sec:technical-background}

    Be brief.

    Overall approach for engineering python environment setup: \cite{conda_forge_community_2015_4774216}, experiment tracking: \cite{wandb}

    <short description of code base, the main classes and their responsibility>

    \subsection*{\nameref{phs:one}}

    \subsubsection*{\nameref{itm:phase1-step1}}
    - Data Investigation in Notebook


    \section{Method}\label{sec:method}

    Description of method\\

    \subsection*{\nameref{phs:one}}

    Overall I'm using the training data for testing and feature engineering, and the test data to make submission.
    However, given there's no true no number of bikes provided for the test data I also take 10\% of the training data away
    for validation purposes.

    Development:
    For development I'm implemeting all code that is not pure experimentation in a test driven way. Further more all parameters are defined in a configurations class. This avoids magic nubmers and
    strings, ensures these values get logged in W&B which helps with reproducibility for each experiment even at this early stage.

    Quite a bit of iteration between step 1 and 2 - I didn't wanted to learn more about the data than I needed to

    \subsubsection*{\nameref{itm:phase1-step1}}
    - Goal is to decide what sort of problem it is (see results)
    - Goal is to find out what's similar and what's different between the different station
    - Can I understand what the data means? What the different columns are by firstly understanding what data I've been given
    (see results for interpretation of the data)
    - Iterating back to this step as I'm building the model and need to understand more about the data


    \subsubsection*{\nameref{itm:phase1-step2}
    Setup
    I wanted to not decide on an algorithm from the get go but instead create a pipeline that let me easily try out and evaluate
    a few models before spending time making one of them good. For this I setup pure Python pipeline:
    : load data, predict random numbers, calculate mean average error, log results both in csv file and to W&B

    Random Guessing
    Firstly, I created a baseline by randomly guessing the number of bikes for each sample porvided in the training data a few
    times.

    Simple regression - one model for all stations
    - Goal: input variable X = number of bikes 3h ago -> predict number of bikes now
    - Second experiment is do a simple regression based on just one feature bikes 3h ago
    - Remove a further random 10\% of the Training data into a valdiation data set so I have some labelled validation data
    - now I have a development and a validation dataset that's labelled
    - 300 rows don/t hve bikes from 3 hours ago, decide what to do, e.g just drop, put to average bikes at station number, put to
    previos bikes 3h ago...
        - I've decided to set the number to the most common number for that column


    \section{Experiment Setup}\label{sec:experiment-setup}
    Overall: I'm using IDEs for coding and running: JetBrains DataSpell for Notebooks and PyCharm for pure Python. Both IDEs offer
    a rich debugging and runtime environment, test and script execution, advanced refactoring abilities and so forth.
    - logging everything including configurations, git revision nubmer to weight and biasess

    \subsection*{\nameref{phs:one}}

    \subsubsection*{\nameref{itm:phase1-step1}}
    Data Experimentation was mostly done in Jupyter notebooks which can be seen on github.
    Data loaded with Pandas Dataframe

    \subsubsection*{\nameref{itm:phase1-step2}
    For this I wrote pure Python. Random guessing is done just picking a random integer between 0 and number of docks
    for each station (using Pythons random library). I've used skikits mean absolute error calculation function. The result
    is logged to W&B.

    Random Guessing
    - Similar data loading technique from step 1 but in pure Python
    - Test drive the classes with dummy data to be able to quickly develop the code
    - Coniguration class for real runs and one for testing  (include link to class or appendix)
        - testing one is not writing to wandb
        - Keeping the dataframes in memory for speed and because they are small enough
        -  reads all csv into one big dataframe and ensures that the index is unique (not like it would be by default) - see Data class

        Simple Regression - one model for all station
        - Write dev and validation csv to make loading of those data sets easier
        - have a script to do this split and it's tested two, I don't rerun it unless I want a new set
        - reuse data reading from Random Guessing
        - unlabelled rows are removed
        - ending up with 50220 samples for taining and 5580 samples for validation for development
        - for nan values in bikes_3h_ago the df column type was changed to be categorical and  used a Simple imputer
        with the most frequent value for that column to replace none zeros

%    TODO paste code


        \section{Results}\label{sec:results}
        Results achieved

        \subsection*{\nameref{phs:one}}
        Overall:

        \subsubsection*{\nameref{itm:phase1-step1}}
        - There are 55875 labelled samples
        - 13275 rows have at least one NanValue -> what to do with nan
        - 75 rows don't have a bike prediction - decided to remove these rows for evaluating the performance of my alogrithm
        - Some interesting data that I didn't know what it mean
        - Describe the columns and what I think they mean
        - Not all data types are usefull in it's current format, e.g Weekday would have to be translated into a numerical value as day of week
        - 300 examples from training datas don't provide bikes from 3 hours ago

        - This is most naturally a multivariate regression problem
        - Multivariate Regression to predict a number but could be a classifier if we say the numbers are from a discrete set of numbers
        - Supervised learning given we've got labelled data
        - Might be worth to do some feature exploring with clustering to learn how the different features relate to each other
        - Given that the goal is to predict a number and given I have labelled data for some stations I decided that this could be
            well formulated as a regression problem.

        \subsubsection*{\nameref{itm:phase1-step2}

        Random Guessing - the same if one model for all stations or not
        - for average see wandb, average random guessing had a mean average error of 7.307-7.369

        Simple Regression - one model for all stations
        - dev dataset is now of size 50220 and validation data set is 5580
        - Mean average error for my validation test set is 3.812
        - Mean average error for dev set I trained with is 3.767
        - Significantly better than random guessing



        \section{Conclusions}\label{sec:conclusions}
        share any insights you gained about how the system works.


        \section*{Acknowledgements}

        \bibliographystyle{plain}
        \bibliography{bibliography}
%TODO reference the varsious medium articles for software engineering
\end{document}
