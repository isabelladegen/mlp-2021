%! Author = idegen
%! Date = 26/11/2021

\documentclass{article}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[default,oldstyle,scale=0.95]{opensans}
\usepackage[]{amsthm}
\usepackage[]{amssymb}
\usepackage{placeins}
\usepackage{textcomp}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage[useregional]{datetime2}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=Emerald,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=RoyalBlue
}
\makeatletter
\def\namedlabel#1#2{\begingroup
#2%
\def\@currentlabel{#2}%
\phantomsection\label{#1}\endgroup
}
\makeatother

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

\pagestyle{fancy}
\fancyhf{}
\chead{Machine Learning Paradigms}
\rhead{\today}
\lhead{Isabella Degen}
\lfoot{isabella.degen@bristol.ac.uk}
\rfoot{Page \thepage}

\title{Machine Learning Paradigms - Coursework Report}
\author{Isabella Degen - Interactive AI CDT}

\begin{document}
    \maketitle


    \section{Introduction}\label{sec:introduction}
    Motivation and Approach %TODO: delete
    My goal for this coursework was to gain more experience with the various ML algorithms and evaluation methods we've
    seen during the semester, various different python frameworks available, methods to keep track of experiments, as
    well as learning more about how to apply good software engineering practices I'm used to from industry to a ML problem.
    I was less focused on finding the best solution or winning the competition.

    I followed the steps laid out on \href{https://www.kaggle.com/c/morebikes2021/overview/description}{Kaggle} and broke
    them down into the following sub steps:

    \subsection*{\label{phs:one}{Phase 1}}
    Predictions of station's 201-275 availabilities for one month by (a) training a model per station or (b)
    training one model for all stations.

    \begin{description}
        \item [\namedlabel{itm:phase1-step1}{Step 1}] Analyse and investigate station's test data and Decide on what problem it is. Classification (binary, multi label), Regression, ...
        \item [\namedlabel{itm:phase1-step2}{Step 2}] Experiment with a few models using approach a (training model per station)
        \item [\namedlabel{itm:phase1-step3}{Step 3}] Experiment with a few models using approach b (training one model for all station)
        \item [\namedlabel{itm:phase1-step4}{Step 4}] Tune the more promising potential solutions
        \item [\namedlabel{itm:phase1-step5}{Step 5}] Submit results
    \end{description}

    \subsection*{\label{phs:two}{Phase 2}}
    Use the pre-trained models for station 1-200 to predict availability for stations 201-275
    \begin{description}
        \item [\namedlabel{itm:phase2-step1}{Step 1}] Analyse given models
        \item [\namedlabel{itm:phase2-step2}{Step 2}] Experiment with a few approaches
        \item [\namedlabel{itm:phase2-step3}{Step 3}] Tune the more promising potential solutions
        \item [\namedlabel{itm:phase2-step4}{Step 4}] Investigate performance
        \item [\namedlabel{itm:phase2-step5}{Step 5}] Submit results
        \item [\namedlabel{itm:phase2-step6}{Step 6}] Compare with results from \nameref{phs:one}
    \end{description}

    \subsection*{\label{phs:three}{Phase 3}}
    Combine approaches from \nameref{phs:one} and \nameref{phs:two}
    \begin{description}
        \item [\namedlabel{itm:phase3-step1}{Step 1}] TODO

    \end{description}
    I keep referring back to these steps in section: \nameref{sec:method}, \nameref{sec:technical-background}, \nameref{sec:experiment-setup},
    \nameref{sec:results}. The final sections \nameref{sec:conclusions} gives a summary of my overall learnings and conclusions.

    %TODO reference to github
    %TODO reference interactive to weight and biases graphs, include static version


    \section{Technical Background}\label{sec:technical-background}

    Be brief.

    Overall approach for engineering python environment setup: \cite{conda_forge_community_2015_4774216}, experiment tracking: \cite{wandb}

    <short description of code base, the main classes and their responsibility>

    \subsection*{\nameref{phs:one}}

    \subsubsection*{\nameref{itm:phase1-step1}}
    - Data Investigation in Notebook


    \section{Method}\label{sec:method}

    Description of method\\

    \subsection*{\nameref{phs:one}}

    Overall I'm using the training data for testing and feature engineering, and the test data to make submission.
    However, given there's no true no number of bikes provided for the test data I also take 10\% of the training data away
    for validation purposes.

    Development:
    For development I'm implemeting all code that is not pure experimentation in a test driven way. Further more all parameters are defined in a configurations class. This avoids magic nubmers and
    strings, ensures these values get logged in W&B which helps with reproducibility for each experiment even at this early stage.

    Quite a bit of iteration between step 1 and 2 - I didn't wanted to learn more about the data than I needed to

    \subsubsection*{\nameref{itm:phase1-step1}}
    - Goal is to decide what sort of problem it is (see results)
    - Goal is to find out what's similar and what's different between the different station
    - Can I understand what the data means? What the different columns are by firstly understanding what data I've been given
    (see results for interpretation of the data)
    - Iterating back to this step as I'm building the model and need to understand more about the data


    \subsubsection*{\nameref{itm:phase1-step2}
    Setup
    I wanted to not decide on an algorithm from the get go but instead create a pipeline that let me easily try out and evaluate
    a few models before spending time making one of them good. For this I setup pure Python pipeline:
    : load data, predict random numbers, calculate mean average error, log results both in csv file and to W&B

    Random Guessing
    Firstly, I created a baseline by randomly guessing the number of bikes for each sample porvided in the training data a few
    times.

    Simple regression - one model for all stations
    - Goal: input variable X = number of bikes 3h ago -> predict number of bikes now
    - Second experiment is do a simple regression based on just one feature bikes 3h ago
    - Remove a further random 10\% of the Training data into a valdiation data set so I have some labelled validation data
    - now I have a development and a validation dataset that's labelled
    - 300 rows don/t hve bikes from 3 hours ago, decide what to do, e.g just drop, put to average bikes at station number, put to
    previos bikes 3h ago...
        - Set nan to the most common number for that column (experiment) - see get feature matrix in Data.py

    Simple Regression - model per station
    - same as above but train a different model for each station
    - using the same model class (PoissonModel) but having another class that can be given whatever model and that manages
        the data to be split per station
    - also calculate mae for each station as well as overall both for both approaches


    \section{Experiment Setup}\label{sec:experiment-setup}
    Overall: I'm using IDEs for coding and running: JetBrains DataSpell for Notebooks and PyCharm for pure Python. Both IDEs offer
    a rich debugging and runtime environment, test and script execution, advanced refactoring abilities and so forth.
    - logging everything including configurations, git revision nubmer to weight and biasess

    \subsection*{\nameref{phs:one}}

    \subsubsection*{\nameref{itm:phase1-step1}}
    Data Experimentation was mostly done in Jupyter notebooks which can be seen on github.
    Data loaded with Pandas Dataframe

    \subsubsection*{\nameref{itm:phase1-step2}
    For this I wrote pure Python. Random guessing is done just picking a random integer between 0 and number of docks
    for each station (using Pythons random library). I've used skikits mean absolute error calculation function. The result
    is logged to W&B.

    Random Guessing
    - Similar data loading technique from step 1 but in pure Python
    - Test drive the classes with dummy data to be able to quickly develop the code
    - Configuration class for real runs and one for testing  (include link to class or appendix)
        - testing one is not writing to wandb
        - Keeping the dataframes in memory for speed and because they are small enough
        -  reads all csv into one big dataframe and ensures that the index is unique (not like it would be by default) - see Data class

        Simple Regression - one model for all station
        - Write dev and validation csv to make loading of those data sets easier
        - have a script to do this split and it's tested two, I don't rerun it unless I want a new set
        - reuse data reading from Random Guessing
        - unlabelled rows are removed
        - ending up with 50220 samples for taining and 5580 samples for validation for development
        - for nan values in bikes_3h_ago the df column type was changed to be categorical and  used a Simple imputer
        with the most frequent value for that column to replace none zeros

%    TODO paste code

        Simple Regression - model per station
        - simple_regression.py will run both the single model and the model per station training and prediction, the parameters are logged
            accordingly
        - adjusted PredictionResults.py to be able to be constructed from multiple (per station results)
        - log an additional table with mae per station for both one model for all station as well as for the model per station
        - tried both one and two features: bikes_3h_ago, numDocks as well as just bikes_3h_ago

        Random Forest Regressor
        - Reuse all the previous code but implement new parameters

        \section{Results}\label{sec:results}
        Results achieved

        \subsection*{\nameref{phs:one}}
        Overall:

        \subsubsection*{\nameref{itm:phase1-step1}}
        - There are 55875 labelled samples
        - 13275 rows have at least one NanValue -> what to do with nan
        - 75 rows don't have a bike prediction - decided to remove these rows for evaluating the performance of my alogrithm
        - Some interesting data that I didn't know what it mean
        - Describe the columns and what I think they mean
        - Not all data types are usefull in it's current format, e.g Weekday would have to be translated into a numerical value as day of week
        - 300 examples from training datas don't provide bikes from 3 hours ago

        - This is most naturally a multivariate regression problem
        - Multivariate Regression to predict a number but could be a classifier if we say the numbers are from a discrete set of numbers
        - Supervised learning given we've got labelled data
        - Might be worth to do some feature exploring with clustering to learn how the different features relate to each other
        - Given that the goal is to predict a number and given I have labelled data for some stations I decided that this could be
            well formulated as a regression problem.

        \subsubsection*{\nameref{itm:phase1-step2}

        Random Guessing - the same if one model for all stations or not
        - for average see wandb, average random guessing had a mean average error of 7.307-7.369

        Simple Regression - one model for all stations
        - dev dataset is now of size 50220 and validation data set is 5580
        - Significantly better than random guessing

        Simple Regression - model per station
        - same data set as above

            Features = Bikes_3h_ago
            One model - training data mae = 3.767
            One model - validation data mae = 3.812
            per station - training data mae = 3.236
            per station - validation data mae = 3.327

            Features = Bikes_3h_ago & num Docks
            One model - training data mae = 3.661
            One model - validation data mae = 3.706
            per station - training data mae = 3.236
            per station - validation data mae = 3.327

        -> Model per station performs slightly better with minimal additional time for the single model
        -> no difference in features used for model per station

        -> overall the model per station performed better

        Random Forest Regressor:
        First run: one feature 3h_bikes - 4th peach-bird
        Model per station - Training data: MAE 2.90715
        Model per station - Validation data: MAE 3.14552
        One model - Training data: MAE 3.19787
        One model - Validation data: MAE 3.28136

        Second run: 2 features num of Docks and 3h_bikes - 4th autumn-rain
        Model per station - Training data: MAE 2.90709
        Model per station - Validation data: MAE 3.14749
        One model - Training data: MAE 3.12366
        One model - Validation data: MAE 3.26918

        One Model:
        Station differences:
        -> Random Forest does better for all stations, so far all algorithms struggle with a few stations:
        - station 233 - MAE 11.6 for PoissonRegressor (2 features - classic pyramid - training), ~8 for Random Forest * both (particularly in training ata)
        - station 249 - MAE 11.25 Poisson (classic pyramid - validation), 8.1 Random Forest (Peach bird - validation),
                        6.3 (Peach  bird - training)
        - station 208 - MAE 7.7 Poissn classic pyramid validation, 6.71 Random Forest Peach bird - validation
        - station 230 - MAE 7.6 Poisson classic pyramid validation, 7.1 Random Forest (peach bird - validation)

        -> A few stations do better than average
        - station 209-211, 213, 227, 252, 256-259, 262, 265-270, 272-275 all have MAE around 2 for Random Forest

        Differences in Training and Validation:
        - Most stations do worse in validation as expected given the alorithm is trained on the training data, however for station
          233 this is not the case it does worse in training (11.6training, 10.4 validation poisson-classic pyramid),
            (8.1 training, 7.6 validation peach bird -  random forest)

        For multiple models these trends are the same (the numbers are slightly different)
        - Multiple models perform slightly better (see weave)

        \section{Conclusions}\label{sec:conclusions}
        share any insights you gained about how the system works.


        \section*{Acknowledgements}

        \bibliographystyle{plain}
        \bibliography{bibliography}
%TODO reference the varsious medium articles for software engineering
\end{document}
